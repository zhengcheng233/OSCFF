         :-) GROMACS - gmx mdrun, 2023.2-20231019-33a0c9850-unknown (-:

Executable:   /public/home/chengz/apprepo/gromacs_threadMPI/2023.2-dtk23.10_hpcx241/app/bin/gmx
Data prefix:  /public/home/chengz/apprepo/gromacs_threadMPI/2023.2-dtk23.10_hpcx241/app
Working dir:  /tmp/scrtch/chengz/gro_75187919/elec
Command line:
  gmx mdrun -gpu_id 0 -ntmpi 1 -ntomp 7 -nb gpu -pme gpu -deffnm md

Reading file md.tpr, VERSION 2023.2-20231019-33a0c9850-unknown (single precision)
Changing nstlist from 10 to 100, rlist from 1 to 1.081

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the GPU
PME tasks will do all aspects on the GPU
Using 1 MPI thread
Using 7 OpenMP threads 


NOTE: The number of threads is not equal to the number of (logical) cpus
      and the -pin option is set to auto: will not pin threads to cpus.
      This can lead to significant performance degradation.
      Consider using -pin on (and -pinoffset in case you run multiple jobs).
starting mdrun 'mol'
1000000 steps,   2000.0 ps.

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:     2760.927      394.419      700.0
                 (ns/day)    (hour/ns)
Performance:      438.113        0.055

GROMACS reminds you: "Never mind, death professor, your structure's fine" (TV on the Radio)

